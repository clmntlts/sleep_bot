import streamlit as st
import google.generativeai as genai
import requests
import datetime # Importation ajout√©e

# üëâ Configuration de la page
st.set_page_config(page_title="Chatbot Gemini", layout="centered")
st.title("ü§ñ Sleep bot - Made in Cl√©ment")

# üëâ Barre lat√©rale : Configuration
st.sidebar.header("üîê Configuration")
api_key = st.sidebar.text_input("Cl√© API Gemini", type="password", placeholder="Collez votre API Key ici")

# üëâ Param√®tres du mod√®le
st.sidebar.header("‚öôÔ∏è Param√®tres du mod√®le")
temperature = st.sidebar.slider("Temp√©rature (cr√©ativit√©)", 0.0, 1.0, 0.7)
max_tokens = st.sidebar.slider("Longueur max de r√©ponse", 100, 4096, 2048) # Augment√©
top_p = st.sidebar.slider("Top-p", 0.0, 1.0, 1.0)
top_k = st.sidebar.slider("Top-k", 1, 100, 40)

# üëâ Param√®tres g√©n√©raux
DEFAULT_PROMPT = "Tu es un assistant psychologue expert du sommeil sp√©cialis√© dans la Th√©rapie Cognitive et Comportementale pour l'Insomnie (TCC-I). Ton r√¥le est de guider un patient √† travers un entretien clinique initial structur√©. Suis attentivement les instructions de section."
GITHUB_PROMPT_URL = "https://raw.githubusercontent.com/clmntlts/sleep_bot/main/sleep_prompt.txt" # Assurez-vous que ce prompt est complet
MODEL_NAMES = ["gemini-1.5-pro-latest", "gemini-1.5-flash-latest"]

INTERVIEW_SECTIONS = {
    1: "Motif de consultation et plainte principale",
    2: "Caract√©ristiques d√©taill√©es de l‚Äôinsomnie",
    3: "Cons√©quences diurnes et somnolence",
    4: "Hygi√®ne de sommeil, environnement et rythmes",
    5: "Facteurs cognitifs et √©motionnels",
    6: "Facteurs comportementaux",
    7: "Ant√©c√©dents m√©dicaux et psychiatriques",
    8: "Histoire du sommeil et traitements ant√©rieurs",
    9: "Contexte psychosocial et stresseurs",
    10: "Attentes et motivation"
}

if "interview_section" not in st.session_state:
    st.session_state.interview_section = 1
if "interview_complete" not in st.session_state:
    st.session_state.interview_complete = False


@st.cache_data(show_spinner="üîÑ T√©l√©chargement du prompt depuis GitHub...")
def fetch_prompt(url):
    try:
        response = requests.get(url)
        response.raise_for_status()
        st.success("Prompt de l'entretien charg√© depuis GitHub.")
        return response.text
    except Exception as e:
        st.warning(f"Impossible de charger le prompt depuis GitHub ({e}). Utilisation du prompt par d√©faut.")
        return DEFAULT_PROMPT

# MODIFI√â: init_gemini_model prend system_instruction
# RETIR√â: @st.cache_resource car system_instruction est dynamique
def init_gemini_model(model_name, system_instruction_for_model):
    # st.write(f"DEBUG: Initialisation du mod√®le {model_name} avec instruction syst√®me.") # Pour d√©bogage
    try:
        model = genai.GenerativeModel(
            model_name=model_name,
            system_instruction=system_instruction_for_model # Param√®tre correct
        )
        return model
    except Exception as e:
        st.error(f"Erreur lors de l'initialisation de GenerativeModel ({model_name}): {e}")
        st.exception(e)
        return None

def export_conversation_as_markdown(history):
    today = datetime.date.today()
    md = f"# üß† Historique de Conversation - {today}\n\n"
    if not history: return "L'historique de la conversation est vide."
    for message in history:
        role = "üë§ Patient" if message.role == "user" else "ü§ñ Assistant IA"
        text_content = message.parts[0].text if message.parts else "[Message vide]"
        md += f"**{role}:**\n{text_content}\n\n---\n\n"
    return md

def generate_clinical_summary(chat_history, model_instance, generation_config_for_summary):
    if not chat_history: return "Impossible de g√©n√©rer le rapport : l'historique est vide."
    conversation_text = ""
    for message in chat_history:
        role = "Patient" if message.role == "user" else "Assistant IA"
        text_content = message.parts[0].text if message.parts else ""
        conversation_text += f"{role}: {text_content}\n\n"

    summary_prompt_text = f"""
**Prompt pour G√©n√©ration de Compte Rendu Clinique TCC-I**
**Votre R√¥le :** Vous √™tes un assistant IA charg√© de r√©diger un compte rendu clinique structur√©.
**T√¢che :** En vous basant **exclusivement** sur l'historique de la conversation d'entretien clinique TCC-I fourni ci-dessous, g√©n√©rez un compte rendu synth√©tique, d√©taill√© et organis√© destin√© √† un th√©rapeute qualifi√©.
**Format de Sortie Attendu :** Le compte rendu doit **strictement** suivre la structure suivante, en extrayant et synth√©tisant les informations pertinentes de la conversation pour chaque section :

1.  **Identifiant Patient (si disponible dans l'historique, sinon omettre)**
2.  **Motif de Consultation**
3.  **Histoire de l'Insomnie & Caract√©ristiques Actuelles** (incluant type, d√©but, dur√©e, horaire type semaine/weekend, latence, r√©veils, TST estim√©, qualit√©/satisfaction subjective, variabilit√©, perception par autrui)
4.  **Facteurs Cognitifs** (inqui√©tudes sp√©cifiques, peurs sur cons√©quences, attentes, rumination, attribution, perception contr√¥le, effort/lutte, √©motions associ√©es, niveau de pr√©occupation)
5.  **Facteurs Comportementaux** (actions pendant l'√©veil nocturne, description d'une nuit type, consultation de l'heure, comportements compensatoires)
6.  **Hygi√®ne de Sommeil, Environnement & Rythmes** (routine, environnement, usage du lit, consommation substances, activit√© physique, alimentation, r√©gularit√© horaires, lumi√®re)
7.  **Cons√©quences Diurnes & Somnolence** (impact sur fonctionnement, √©chelle d'impact, √©valuation type ESS si disponible, siestes)
8.  **Ant√©c√©dents M√©dicaux, Psychiatriques & Autres Troubles du Sommeil** (conditions m√©dicales, suivi psy, m√©dicaments/compl√©ments, facteurs f√©minins, d√©pistage SAOS/SJSR bas√© sur les r√©ponses)
9.  **Traitements Ant√©rieurs** (exp√©riences pass√©es avec traitements)
10. **Contexte Psychosocial & Stresseurs Actuels** (niveau de stress, sources, gestion stress, soutien social)
11. **Attentes & Motivation** (attentes TCC-I, objectifs concrets, √©chelle motivation, craintes/h√©sitations)
12. **Synth√®se des Facteurs de Maintien Propos√©s** (r√©sum√© des hypoth√®ses sur les facteurs cognitifs, comportementaux, circadiens etc. qui entretiennent l'insomnie, bas√© sur l'analyse de la conversation)
13. **Niveau de Motivation & Compr√©hension** (√©valuation subjective bas√©e sur les dires du patient)
14. **Drapeaux Rouges / Points d'Attention** (mention de toute suspicion SAOS/SJSR, sympt√¥mes anxio-d√©pressifs marqu√©s, id√©es suicidaires si exprim√©es, abus de substance, ou tout autre point n√©cessitant une vigilance clinique particuli√®re)

**Instructions Importantes :**
* **Basez-vous UNIQUEMENT sur le contenu de la conversation fournie.** N'inventez aucune information. Si une information manque pour une section, indiquez "Non abord√©" ou "Information non disponible dans l'historique".
* Adoptez un ton clinique, professionnel et objectif.
* Soyez synth√©tique tout en restant informatif.
* Respectez scrupuleusement la structure demand√©e.
**Historique de la Conversation √† Synth√©tiser :**
--- D√âBUT HISTORIQUE ---
{conversation_text}
--- FIN HISTORIQUE ---"""
    try:
        response = model_instance.generate_content(
            summary_prompt_text,
            generation_config=generation_config_for_summary
        )
        summary_text = response.parts[0].text if response.parts else ""
        if not summary_text and hasattr(response, 'prompt_feedback') and response.prompt_feedback.block_reason:
            return f"Erreur : La g√©n√©ration du rapport a √©t√© bloqu√©e. Raison : {response.prompt_feedback.block_reason}"
        return summary_text if summary_text else "Le mod√®le n'a pas pu g√©n√©rer de r√©sum√©."
    except Exception as e:
        print(f"Erreur lors de l'appel √† generate_content pour le r√©sum√© : {e}")
        return f"Erreur technique lors de la g√©n√©ration du compte rendu : {e}"

# üëâ Lancement principal
if not api_key:
    st.warning("Veuillez entrer votre cl√© API Gemini dans la barre lat√©rale.")
    st.stop()

try:
    genai.configure(api_key=api_key)
    selected_model_name = st.sidebar.selectbox("üß† Choisir le mod√®le", MODEL_NAMES, index=0)
    interview_prompt_base = fetch_prompt(GITHUB_PROMPT_URL)

    # Auto-introduction par le bot en d√©but de section
    if not st.session_state.interview_complete and st.session_state.get("awaiting_bot_intro", True):
        with st.chat_message("model"):
            intro_prompt = f"Commen√ßons la section {st.session_state.interview_section} : '{INTERVIEW_SECTIONS[st.session_state.interview_section]}'. Peux-tu me parler de cela ?"
            response = st.session_state.chat.send_message(intro_prompt, generation_config=generation_config)
            st.markdown(response.text if hasattr(response, "text") else "[R√©ponse vide]")
        st.session_state.awaiting_bot_intro = False


    if not st.session_state.interview_complete:
        current_section_number = st.session_state.interview_section
        current_section_title = INTERVIEW_SECTIONS.get(current_section_number, "Section inconnue")
        section_instruction = f"\n\nüéØ Nous sommes actuellement dans la section {current_section_number} : '{current_section_title}'. Concentre tes questions sur cette section sp√©cifique pour le moment. Guide l'utilisateur √† travers cette section."
        current_full_system_prompt = interview_prompt_base + section_instruction
    else:
        current_full_system_prompt = interview_prompt_base + "\n\n L'entretien est termin√©. Remercie l'utilisateur et indique qu'il peut t√©l√©charger les rapports."

    should_reinitialize_model_and_chat = (
        "gemini_model_instance" not in st.session_state or
        st.session_state.get("current_model_name") != selected_model_name or
        st.session_state.get("system_prompt_in_use") != current_full_system_prompt
    )

    if should_reinitialize_model_and_chat:
        # st.write("DEBUG: R√©initialisation du mod√®le et/ou de la session de chat...")
        st.session_state.gemini_model_instance = init_gemini_model(selected_model_name, current_full_system_prompt)
        if st.session_state.gemini_model_instance is None: # V√©rifier si l'init a √©chou√©
            st.error("√âchec de l'initialisation du mod√®le. V√©rifiez la console pour les erreurs.")
            st.stop()

        st.session_state.current_model_name = selected_model_name
        st.session_state.system_prompt_in_use = current_full_system_prompt
        
        history_for_new_chat = []
        # Conserver l'historique si le nom du mod√®le n'a pas chang√©
        if "chat" in st.session_state and hasattr(st.session_state.chat, 'history') and st.session_state.get("current_model_name") == selected_model_name:
            history_for_new_chat = st.session_state.chat.history
        
        st.session_state.chat = st.session_state.gemini_model_instance.start_chat(
            history=history_for_new_chat # PAS d'argument system_prompt ici
        )
        # st.success(f"Mod√®le {selected_model_name} pr√™t. Section : {st.session_state.get('interview_section',1)}")


    generation_config = genai.types.GenerationConfig(
        temperature=temperature, max_output_tokens=max_tokens, top_p=top_p, top_k=top_k
    )

    if not st.session_state.interview_complete:
        current_section_title = INTERVIEW_SECTIONS.get(st.session_state.interview_section, "Termin√©")
        st.markdown(f"### üìã Section {st.session_state.interview_section} / {len(INTERVIEW_SECTIONS)} : {current_section_title}")
        st.progress(st.session_state.interview_section / len(INTERVIEW_SECTIONS))

    if "chat" in st.session_state:
        for message in st.session_state.chat.history:
            role_display = "user" if message.role == "user" else "model"
            with st.chat_message(role_display):
                st.markdown(message.parts[0].text if message.parts else "[Message vide]")

    user_input = None
    if not st.session_state.interview_complete:
        user_input = st.chat_input("R√©pondez ici...")

    if user_input and "chat" in st.session_state:
        with st.chat_message("user"): st.markdown(user_input)
        with st.chat_message("model"):
            message_placeholder = st.empty()
            try:
                response = st.session_state.chat.send_message(user_input, generation_config=generation_config)
                response_text = response.parts[0].text if response.parts else ""
                if not response_text and hasattr(response, 'prompt_feedback') and response.prompt_feedback.block_reason:
                    response_text = f"‚ö†Ô∏è R√©ponse bloqu√©e. Raison : {response.prompt_feedback.block_reason}"
                message_placeholder.markdown(response_text)
            except Exception as e:
                st.error(f"Erreur lors de l'envoi du message : {e}")

    col_action1, col_action2 = st.columns([3,1])
    with col_action1:
        if not st.session_state.interview_complete:
            if st.session_state.interview_section < len(INTERVIEW_SECTIONS):
                if st.button(f"‚úÖ Terminer section {st.session_state.interview_section} et passer √† la suivante", use_container_width=True):
                    st.session_state.interview_section += 1
                    st.session_state.awaiting_bot_intro = True
                    st.rerun()
            elif st.session_state.interview_section == len(INTERVIEW_SECTIONS):
                if st.button("üèÅ Terminer l'entretien", use_container_width=True):
                    st.session_state.interview_complete = True
                    st.rerun()
    
    with col_action2:
        if st.button("üîÑ R√©initialiser", use_container_width=True):
            keys_to_reset = ["chat", "interview_section", "interview_complete", "gemini_model_instance", "current_model_name", "system_prompt_in_use"]
            for key in keys_to_reset:
                if key in st.session_state: del st.session_state[key]
            st.cache_data.clear(); st.cache_resource.clear()
            st.success("Entretien r√©initialis√©."); st.rerun()

    if st.session_state.interview_complete:
        st.success("üéâ Entretien termin√© ! Vous pouvez maintenant t√©l√©charger les rapports.")
        st.markdown("---"); st.subheader("T√©l√©chargement des Rapports")
        col_dl1, col_dl2 = st.columns(2)
        with col_dl1:
            if "chat" in st.session_state and st.session_state.chat.history:
                raw_history_md = export_conversation_as_markdown(st.session_state.chat.history)
                st.download_button(label="üì• T√©l√©charger l'Historique (.md)", data=raw_history_md.encode("utf-8"), file_name=f"historique_entretien_{datetime.date.today()}.md", mime="text/markdown", key="download_raw_history")
            else: st.info("Aucun historique √† t√©l√©charger.")
        with col_dl2:
            if "chat" in st.session_state and st.session_state.chat.history and "gemini_model_instance" in st.session_state:
                with st.spinner("G√©n√©ration du compte rendu clinique..."):
                    clinical_summary_md = generate_clinical_summary(st.session_state.chat.history, st.session_state.gemini_model_instance, generation_config)
                if "Erreur" in clinical_summary_md: st.error(clinical_summary_md)
                else: st.download_button(label="üì• T√©l√©charger le Compte Rendu (.md)", data=clinical_summary_md.encode("utf-8"), file_name=f"compte_rendu_clinique_{datetime.date.today()}.md", mime="text/markdown", key="download_summary_report")
            elif not ("chat" in st.session_state and st.session_state.chat.history) : st.info("Aucun historique pour g√©n√©rer un compte rendu.")
            else: st.warning("Mod√®le IA non initialis√©, compte rendu impossible.")

except Exception as e:
    st.error(f"‚ùå Une erreur majeure est survenue : {e}")
    st.exception(e)