import streamlit as st
import google.generativeai as genai
import requests
import datetime # Importation ajout√©e

# üëâ Configuration de la page
st.set_page_config(page_title="Chatbot Gemini", layout="centered")
st.title("ü§ñ Sleep bot - Made in Cl√©ment")

# üëâ Barre lat√©rale : Configuration
st.sidebar.header("üîê Configuration")
api_key = st.sidebar.text_input("Cl√© API Gemini", type="password", placeholder="Collez votre API Key ici")

# üëâ Param√®tres du mod√®le
st.sidebar.header("‚öôÔ∏è Param√®tres du mod√®le")
temperature = st.sidebar.slider("Temp√©rature (cr√©ativit√©)", 0.0, 1.0, 0.7)
max_tokens = st.sidebar.slider("Longueur max de r√©ponse", 100, 2048, 1024) # Augment√© pour permettre des rapports plus longs
top_p = st.sidebar.slider("Top-p", 0.0, 1.0, 1.0)
top_k = st.sidebar.slider("Top-k", 1, 100, 40)

# üëâ Param√®tres g√©n√©raux
DEFAULT_PROMPT = "Tu es un assistant psychologue expert du sommeil"
# Assurez-vous que ce prompt contient les instructions d√©taill√©es pour mener l'entretien
# comme celui fourni dans les messages pr√©c√©dents.
GITHUB_PROMPT_URL = "https://raw.githubusercontent.com/clmntlts/sleep_bot/main/sleep_prompt.txt"
MODEL_NAMES = ["gemini-1.5-pro-latest", "gemini-1.5-flash-latest"] # Mod√®les recommand√©s pour de longues conversations/synth√®ses

# üëâ D√©finition des sections de l'entretien
INTERVIEW_SECTIONS = {
    1: "Motif de consultation et plainte principale",
    2: "Caract√©ristiques d√©taill√©es de l‚Äôinsomnie",
    3: "Cons√©quences diurnes et somnolence",
    4: "Hygi√®ne de sommeil, environnement et rythmes",
    5: "Facteurs cognitifs et √©motionnels",
    6: "Facteurs comportementaux",
    7: "Ant√©c√©dents m√©dicaux et psychiatriques",
    8: "Histoire du sommeil et traitements ant√©rieurs",
    9: "Contexte psychosocial et stresseurs",
    10: "Attentes et motivation"
}

# üëâ Initialisation de l'√©tat de l'entretien
if "interview_section" not in st.session_state:
    st.session_state.interview_section = 1

if "interview_complete" not in st.session_state:
    st.session_state.interview_complete = False

# üëâ Mise en cache du prompt
@st.cache_data(show_spinner="üîÑ T√©l√©chargement du prompt depuis GitHub...")
def fetch_prompt(url):
    try:
        response = requests.get(url)
        response.raise_for_status()
        st.success("Prompt charg√© depuis GitHub.")
        return response.text
    except Exception as e:
        st.warning(f"Impossible de charger le prompt depuis GitHub ({e}). Utilisation du prompt par d√©faut.")
        return DEFAULT_PROMPT

# üëâ Initialisation du mod√®le avec cache
# Note: La configuration de g√©n√©ration est appliqu√©e lors de l'appel, pas √† l'init.
# Le system_instruction peut changer, donc on ne le cache pas ici directement.
@st.cache_resource(show_spinner="üîÑ Initialisation du mod√®le...")
def init_gemini_model(model_name):
     # L'instruction syst√®me sera d√©finie lors du d√©marrage du chat
    return genai.GenerativeModel(model_name=model_name)

# üëâ Fonction export markdown (adapt√©e √† la structure de l'historique Gemini)
def export_conversation_as_markdown(history):
    today = datetime.date.today()
    md = f"# üß† Historique de Conversation - {today}\n\n"
    if not history:
        return "L'historique de la conversation est vide."
    for message in history:
        # Utilisation de 'user' et 'model' comme r√¥les standards de l'API Gemini
        role = "üë§ Patient" if message.role == "user" else "ü§ñ Assistant IA"
        # V√©rifier si parts existe et n'est pas vide
        text_content = ""
        if message.parts:
            # Prendre le texte de la premi√®re partie (la plus courante)
            text_content = message.parts[0].text
        md += f"**{role}:**\n{text_content}\n\n---\n\n"
    return md

# üëâ NOUVELLE FONCTION POUR G√âN√âRER LE COMPTE RENDU STRUCTUR√â
def generate_clinical_summary(chat_history, model, generation_config):
    if not chat_history:
        return "Impossible de g√©n√©rer le rapport : l'historique est vide."

    # 1. Formater l'historique pour l'envoyer au mod√®le
    conversation_text = ""
    for message in chat_history:
        role = "Patient" if message.role == "user" else "Assistant IA"
        text_content = ""
        if message.parts:
            text_content = message.parts[0].text
        conversation_text += f"{role}: {text_content}\n\n"

    # 2. D√©finir le prompt de synth√®se
    summary_prompt = f"""
**Prompt pour G√©n√©ration de Compte Rendu Clinique TCC-I**

**Votre R√¥le :** Vous √™tes un assistant IA charg√© de r√©diger un compte rendu clinique structur√©.

**T√¢che :** En vous basant **exclusivement** sur l'historique de la conversation d'entretien clinique TCC-I fourni ci-dessous, g√©n√©rez un compte rendu synth√©tique, d√©taill√© et organis√© destin√© √† un th√©rapeute qualifi√©.

**Format de Sortie Attendu :** Le compte rendu doit **strictement** suivre la structure suivante, en extrayant et synth√©tisant les informations pertinentes de la conversation pour chaque section :

1.  **Identifiant Patient (si disponible dans l'historique, sinon omettre)**
2.  **Motif de Consultation**
3.  **Histoire de l'Insomnie & Caract√©ristiques Actuelles** (incluant type, d√©but, dur√©e, horaire type semaine/weekend, latence, r√©veils, TST estim√©, qualit√©/satisfaction subjective, variabilit√©, perception par autrui)
4.  **Facteurs Cognitifs** (inqui√©tudes sp√©cifiques, peurs sur cons√©quences, attentes, rumination, attribution, perception contr√¥le, effort/lutte, √©motions associ√©es, niveau de pr√©occupation)
5.  **Facteurs Comportementaux** (actions pendant l'√©veil nocturne, description d'une nuit type, consultation de l'heure, comportements compensatoires)
6.  **Hygi√®ne de Sommeil, Environnement & Rythmes** (routine, environnement, usage du lit, consommation substances, activit√© physique, alimentation, r√©gularit√© horaires, lumi√®re)
7.  **Cons√©quences Diurnes & Somnolence** (impact sur fonctionnement, √©chelle d'impact, √©valuation type ESS si disponible, siestes)
8.  **Ant√©c√©dents M√©dicaux, Psychiatriques & Autres Troubles du Sommeil** (conditions m√©dicales, suivi psy, m√©dicaments/compl√©ments, facteurs f√©minins, d√©pistage SAOS/SJSR bas√© sur les r√©ponses)
9.  **Traitements Ant√©rieurs** (exp√©riences pass√©es avec traitements)
10. **Contexte Psychosocial & Stresseurs Actuels** (niveau de stress, sources, gestion stress, soutien social)
11. **Attentes & Motivation** (attentes TCC-I, objectifs concrets, √©chelle motivation, craintes/h√©sitations)
12. **Synth√®se des Facteurs de Maintien Propos√©s** (r√©sum√© des hypoth√®ses sur les facteurs cognitifs, comportementaux, circadiens etc. qui entretiennent l'insomnie, bas√© sur l'analyse de la conversation)
13. **Niveau de Motivation & Compr√©hension** (√©valuation subjective bas√©e sur les dires du patient)
14. **Drapeaux Rouges / Points d'Attention** (mention de toute suspicion SAOS/SJSR, sympt√¥mes anxio-d√©pressifs marqu√©s, id√©es suicidaires si exprim√©es, abus de substance, ou tout autre point n√©cessitant une vigilance clinique particuli√®re)

**Instructions Importantes :**
* **Basez-vous UNIQUEMENT sur le contenu de la conversation fournie.** N'inventez aucune information. Si une information manque pour une section, indiquez "Non abord√©" ou "Information non disponible dans l'historique".
* Adoptez un ton clinique, professionnel et objectif.
* Soyez synth√©tique tout en restant informatif.
* Respectez scrupuleusement la structure demand√©e.

**Historique de la Conversation √† Synth√©tiser :**
--- D√âBUT HISTORIQUE ---
{conversation_text}
--- FIN HISTORIQUE ---
"""

    # 3. Appeler le mod√®le IA pour g√©n√©rer la synth√®se
    # Note: On utilise le m√™me mod√®le que pour le chat, mais sans l'instruction syst√®me de l'entretien
    # et avec une configuration de g√©n√©ration potentiellement ajust√©e si n√©cessaire (ici on garde la m√™me)
    try:
        # Utilisation de generate_content pour une t√¢che unique (pas de chat continu)
        response = model.generate_content(
            summary_prompt,
            generation_config=generation_config # Utilisation de la config d√©finie par l'utilisateur
            )
        # Extraction correcte pour Gemini API
        summary_text = ""
        if response.parts:
            summary_text = response.parts[0].text
        # V√©rification si le mod√®le a bloqu√© la r√©ponse
        if not summary_text and response.prompt_feedback.block_reason:
             return f"Erreur : La g√©n√©ration du rapport a √©t√© bloqu√©e. Raison : {response.prompt_feedback.block_reason}"
        return summary_text if summary_text else "Le mod√®le n'a pas pu g√©n√©rer de r√©sum√©."

    except Exception as e:
        # Log l'erreur pour le d√©bogage pourrait √™tre utile ici
        print(f"Erreur lors de l'appel √† generate_content pour le r√©sum√© : {e}")
        return f"Erreur technique lors de la g√©n√©ration du compte rendu : {e}"


# üëâ Lancement principal
if not api_key:
    st.warning("Veuillez entrer votre cl√© API Gemini dans la barre lat√©rale.")
    st.stop() # Arr√™ter l'ex√©cution si pas de cl√© API

try:
    genai.configure(api_key=api_key)

    # üëâ Choix du mod√®le
    selected_model_name = st.sidebar.selectbox("üß† Choisir le mod√®le", MODEL_NAMES, index=0)

    # üëâ Chargement du prompt de l'entretien
    interview_prompt_base = fetch_prompt(GITHUB_PROMPT_URL)

    # üëâ Initialisation ou r√©cup√©ration du mod√®le depuis session_state
    # On initialise le mod√®le sans instruction syst√®me ici, elle sera ajout√©e au d√©but du chat
    if "gemini_model" not in st.session_state or st.session_state.model_name != selected_model_name:
        try:
            st.session_state.gemini_model = init_gemini_model(selected_model_name)
            st.session_state.model_name = selected_model_name
            # Supprimer l'ancien chat si le mod√®le change
            if "chat" in st.session_state:
                del st.session_state.chat
            st.success(f"Mod√®le {selected_model_name} initialis√©.")
        except Exception as e:
            st.error(f"Impossible d'initialiser le mod√®le {selected_model_name}: {e}")
            st.stop()

    # üëâ Construction du prompt syst√®me complet pour l'entretien en cours
    if not st.session_state.interview_complete:
        current_section_number = st.session_state.interview_section
        current_section_title = INTERVIEW_SECTIONS.get(current_section_number, "Section inconnue")
        section_instruction = f"\n\nüéØ Nous sommes actuellement dans la section {current_section_number} : '{current_section_title}'. Concentre tes questions sur cette section sp√©cifique pour le moment. Guide l'utilisateur √† travers cette section."
        full_system_prompt_for_interview = interview_prompt_base + section_instruction
    else:
        # Si l'entretien est termin√©, on n'a plus besoin d'instruction de section
        full_system_prompt_for_interview = interview_prompt_base + "\n\n L'entretien est termin√©. Remercie l'utilisateur et indique qu'il peut t√©l√©charger les rapports."


    # üëâ Initialisation du chat avec le prompt syst√®me dynamique
    # On red√©marre le chat si le prompt syst√®me a chang√© (changement de section)
    # ou si le chat n'existe pas
    start_new_chat = False
    if "chat" not in st.session_state:
        start_new_chat = True
    elif "current_system_prompt" not in st.session_state or st.session_state.current_system_prompt != full_system_prompt_for_interview:
         # Si le prompt a chang√© (ex: section suivante), il faut parfois red√©marrer le chat
         # ou au moins mettre √† jour le contexte, selon l'API.
         # Pour Gemini, red√©marrer peut √™tre plus simple pour garantir le nouveau contexte.
         start_new_chat = True # Forcer le red√©marrage pour appliquer le nouveau prompt de section


    if start_new_chat:
        st.session_state.current_system_prompt = full_system_prompt_for_interview
         # D√©marrer un nouveau chat avec le prompt syst√®me actuel et l'historique existant
        history_for_new_chat = st.session_state.chat.history if "chat" in st.session_state else []
        st.session_state.chat = st.session_state.gemini_model.start_chat(
            history=history_for_new_chat # Conserver l'historique existant
            # Note: L'API Gemini actuelle g√®re le system prompt au niveau du mod√®le,
            # mais si on veut le changer dynamiquement, il faut le passer √† send_message
            # ou red√©marrer le chat si n√©cessaire. Ici on le g√®re au niveau du mod√®le si possible
            # ou on l'inclura dans les messages si besoin.
            # Pour l'instant, le prompt est surtout pour guider l'IA.
        )
        # Envoyer un premier message syst√®me pour guider sur la section actuelle (si non premier message)
        # if current_section_number > 1:
            # Pourrait √™tre utile mais peut alourdir l'historique. √Ä tester.
            # st.session_state.chat.send_message(f"Nous passons maintenant √† la section : {current_section_title}", role="system") # Pas un r√¥le standard Gemini

    # üëâ Configuration de g√©n√©ration pour les appels send_message
    generation_config = genai.types.GenerationConfig(
        temperature=temperature,
        max_output_tokens=max_tokens,
        top_p=top_p,
        top_k=top_k,
    )


    # === AFFICHAGE ET INTERACTION ===

    # üëâ Affichage de la section en cours (si entretien non termin√©)
    if not st.session_state.interview_complete:
        current_section_title = INTERVIEW_SECTIONS.get(st.session_state.interview_section, "Termin√©")
        st.markdown(f"### üìã Section {st.session_state.interview_section} / {len(INTERVIEW_SECTIONS)} : {current_section_title}")
        # Barre de progression
        progress_value = st.session_state.interview_section / len(INTERVIEW_SECTIONS)
        st.progress(progress_value)


    # üëâ Affichage de l‚Äôhistorique du chat
    if "chat" in st.session_state:
        for message in st.session_state.chat.history:
            role_display = "user" if message.role == "user" else "model"
            with st.chat_message(role_display):
                text_content = message.parts[0].text if message.parts else "[Message vide]"
                st.markdown(text_content)

    # üëâ Zone de saisie utilisateur (seulement si entretien non termin√©)
    user_input = None
    if not st.session_state.interview_complete:
        user_input = st.chat_input("R√©pondez ici...")

    # üëâ Traitement de la r√©ponse de l'utilisateur et r√©ponse du bot
    if user_input and "chat" in st.session_state:
        # Afficher le message de l'utilisateur
        with st.chat_message("user"):
            st.markdown(user_input)

        # Envoyer le message au mod√®le et afficher la r√©ponse
        with st.chat_message("model"):
            message_placeholder = st.empty()
            try:
                # Construire le message avec le contexte de la section si n√©cessaire
                # (Alternative si le system_prompt n'est pas dynamique)
                # prompt_with_context = f"{full_system_prompt_for_interview}\n\nUtilisateur: {user_input}\n\nAssistant:"
                # response = st.session_state.gemini_model.generate_content(prompt_with_context, generation_config=generation_config)

                # Utilisation de send_message (plus adapt√© pour un chat)
                response = st.session_state.chat.send_message(
                     user_input,
                     generation_config=generation_config,
                     # stream=True # Activer si vous voulez un affichage en streaming
                 )

                # Affichage simple (si stream=False)
                response_text = ""
                if response.parts:
                     response_text = response.parts[0].text
                elif response.prompt_feedback.block_reason:
                     response_text = f"‚ö†Ô∏è R√©ponse bloqu√©e par le mod√®le. Raison : {response.prompt_feedback.block_reason}"

                message_placeholder.markdown(response_text)

                # Affichage en streaming (si stream=True)
                # full_response = ""
                # for chunk in response:
                #     full_response += chunk.text
                #     message_placeholder.markdown(full_response + "‚ñå")
                # message_placeholder.markdown(full_response)


            except Exception as e:
                st.error(f"Erreur lors de l'envoi du message : {e}")
                # Retirer le dernier message utilisateur de l'historique si l'envoi √©choue ?
                # Cela d√©pend de la gestion d'erreur souhait√©e.

    # --- GESTION DE LA FIN DE SECTION ET DE L'ENTRETIEN ---

    # Colonnes pour les boutons d'action en bas
    col_action1, col_action2 = st.columns([3,1]) # Donne plus de place au bouton suivant

    with col_action1:
        # üëâ Bouton pour passer √† la section suivante (affich√© seulement si non termin√©)
        if not st.session_state.interview_complete and st.session_state.interview_section < len(INTERVIEW_SECTIONS):
            if st.button(f"‚úÖ Terminer la section {st.session_state.interview_section} et passer √† la suivante", use_container_width=True):
                st.session_state.interview_section += 1
                # Pas besoin de relancer le chat ici, le prompt sera mis √† jour au prochain rerun
                st.rerun()
        elif not st.session_state.interview_complete and st.session_state.interview_section == len(INTERVIEW_SECTIONS):
             if st.button("üèÅ Terminer l'entretien", use_container_width=True):
                  st.session_state.interview_complete = True
                  # Supprimer le message syst√®me sp√©cifique √† la derni√®re section
                  if "current_system_prompt" in st.session_state:
                      del st.session_state.current_system_prompt
                  st.rerun()

    # üëâ Affichage des boutons de t√©l√©chargement (uniquement si l'entretien est termin√©)
    if st.session_state.interview_complete:
        st.success("üéâ Entretien termin√© ! Vous pouvez maintenant t√©l√©charger les rapports.")
        st.markdown("---")
        st.subheader("T√©l√©chargement des Rapports")

        col_dl1, col_dl2 = st.columns(2)

        with col_dl1:
            # Bouton pour l'historique brut
            if "chat" in st.session_state and st.session_state.chat.history:
                raw_history_md = export_conversation_as_markdown(st.session_state.chat.history)
                st.download_button(
                    label="üì• T√©l√©charger l'Historique (.md)",
                    data=raw_history_md.encode("utf-8"),
                    file_name=f"historique_entretien_{datetime.date.today()}.md",
                    mime="text/markdown",
                    key="download_raw_history" # Cl√© unique
                )
            else:
                st.info("Aucun historique √† t√©l√©charger.")

        with col_dl2:
             # Bouton pour le compte rendu structur√©
            if "chat" in st.session_state and st.session_state.chat.history:
                # S'assurer que le mod√®le est disponible
                if "gemini_model" in st.session_state:
                    with st.spinner("G√©n√©ration du compte rendu clinique..."):
                        # Utiliser la config d√©finie par l'utilisateur pour la g√©n√©ration
                        clinical_summary_md = generate_clinical_summary(
                            st.session_state.chat.history,
                            st.session_state.gemini_model, # Utilise le m√™me mod√®le que le chat
                            generation_config # Utilise la config d√©finie plus haut
                        )

                    if "Erreur" in clinical_summary_md:
                         st.error(clinical_summary_md)
                    else:
                         st.download_button(
                             label="üì• T√©l√©charger le Compte Rendu (.md)",
                             data=clinical_summary_md.encode("utf-8"),
                             file_name=f"compte_rendu_clinique_{datetime.date.today()}.md",
                             mime="text/markdown",
                             key="download_summary_report" # Cl√© unique diff√©rente
                         )
                else:
                    st.warning("Le mod√®le IA n'est pas initialis√©, impossible de g√©n√©rer le compte rendu.")
            else:
                 st.info("Aucun historique disponible pour g√©n√©rer un compte rendu.")


    # üëâ Bouton pour r√©initialiser l'entretien (toujours visible en bas ?)
    # On le met dans la deuxi√®me colonne d'action
    with col_action2:
        if st.button("üîÑ R√©initialiser", use_container_width=True):
            # Liste √©tendue des cl√©s √† potentiellement supprimer
            keys_to_reset = [
                "chat", "interview_section", "interview_complete",
                "gemini_model", "model_name", "current_system_prompt"
            ]
            for key in keys_to_reset:
                if key in st.session_state:
                    del st.session_state[key]
            # Vider le cache de donn√©es et de ressources peut aussi √™tre utile
            st.cache_data.clear()
            st.cache_resource.clear()
            st.success("Entretien r√©initialis√©.")
            st.rerun()


except Exception as e:
    st.error(f"‚ùå Une erreur majeure est survenue : {e}")
    st.exception(e) # Affiche la trace compl√®te pour le d√©bogage